{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import logging\n",
    "from datetime import timezone, timedelta\n",
    "\n",
    "import polyline\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sqlalchemy import create_engine, text, Integer, Float, String, Boolean, DateTime, Interval, Text, BigInteger\n",
    "\n",
    "# Geopy - Nominatim\n",
    "import json, time\n",
    "from tqdm import tqdm\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "# Configuration\n",
    "load_dotenv()\n",
    "\n",
    "# DB\n",
    "DB_URI = os.getenv('DB_URI')\n",
    "\n",
    "# Bronze tables\n",
    "TARGET_B_SCHEMA = os.getenv('TARGET_B_SCHEMA')\n",
    "ACTIVITIES_B_TABLE = os.getenv('ACTIVITIES_B_TABLE')\n",
    "DETAILS_B_TABLE = os.getenv('DETAILS_B_TABLE')\n",
    "KUDOS_B_TABLE = os.getenv('KUDOS_B_TABLE')\n",
    "ZONES_B_TABLE = os.getenv('ZONES_B_TABLE')\n",
    "\n",
    "# Silver tables\n",
    "TARGET_S_SCHEMA = os.getenv('TARGET_S_SCHEMA')\n",
    "ACTIVITIES_S_TABLE = os.getenv('ACTIVITIES_S_TABLE')\n",
    "BEST_EFFORTS_S_TABLE = os.getenv('BEST_EFFORTS_S_TABLE')\n",
    "GEAR_S_TABLE = os.getenv('GEAR_S_TABLE')\n",
    "LAPS_S_TABLE = os.getenv('LAPS_S_TABLE')\n",
    "MAPS_S_TABLE = os.getenv('MAPS_S_TABLE')\n",
    "SEGMENTS_S_TABLE = os.getenv('SEGMENTS_S_TABLE')\n",
    "SEGMENTS_EFFORTS_S_TABLE = os.getenv('SEGMENTS_EFFORTS_S_TABLE')\n",
    "LOCATIONS_S_TABLE = os.getenv('LOCATIONS_S_TABLE')\n",
    "KUDOS_S_TABLE = os.getenv('KUDOS_S_TABLE')\n",
    "ZONES_S_TABLE = os.getenv('ZONES_S_TABLE')\n",
    "RELATIVE_EFFORT_S_TABLE = os.getenv('RELATIVE_EFFORT_S_TABLE')\n",
    "\n",
    "# Other\n",
    "LOG_LEVEL = os.getenv('LOG_LEVEL')\n",
    "NO_GEAR_ID = 'x00000000'\n",
    "\n",
    "# Geopy - Nominatim\n",
    "PRECISION = int(os.getenv('PRECISION'))\n",
    "CACHE_PATH = os.getenv('CACHE_PATH')\n",
    "USER_AGENT = os.getenv('USER_AGENT')\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=getattr(logging, LOG_LEVEL.upper(), logging.INFO),\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    ")\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB names validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_DB_ENV = ['DB_URI', 'TARGET_B_SCHEMA', 'ACTIVITIES_B_TABLE', 'DETAILS_B_TABLE','KUDOS_B_TABLE', 'ZONES_B_TABLE', 'TARGET_S_SCHEMA', 'ACTIVITIES_S_TABLE', 'BEST_EFFORTS_S_TABLE', 'GEAR_S_TABLE', 'LAPS_S_TABLE', 'MAPS_S_TABLE', 'SEGMENTS_S_TABLE', 'SEGMENTS_EFFORTS_S_TABLE', 'ZONES_S_TABLE', 'RELATIVE_EFFORT_S_TABLE']\n",
    "missing_db_env = [env for env in REQUIRED_DB_ENV if not os.getenv(env)]\n",
    "if missing_db_env:\n",
    "  raise RuntimeError(f\"Missing env variables: {', '.join(missing_db_env)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request data from `bronze.activities_details` and `bronze.kudos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\n",
    "  DB_URI, \n",
    "  pool_pre_ping=True, \n",
    "  pool_size=5, \n",
    "  max_overflow=10\n",
    ")\n",
    "logging.info(\"Connection established\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.begin() as conn:\n",
    "  activities_details_df = pd.read_sql(text(f\"SELECT * FROM {TARGET_B_SCHEMA}.{DETAILS_B_TABLE}\"), conn)\n",
    "  kudos_df = pd.read_sql(text(f\"SELECT * FROM {TARGET_B_SCHEMA}.{KUDOS_B_TABLE}\"), conn)\n",
    "  activities_zones_df = pd.read_sql(text(f\"SELECT * FROM {TARGET_B_SCHEMA}.{ZONES_B_TABLE}\"), conn)\n",
    "logging.info(f\"Data from {TARGET_B_SCHEMA}.{DETAILS_B_TABLE}, {TARGET_B_SCHEMA}.{KUDOS_B_TABLE} and {TARGET_B_SCHEMA}.{ZONES_B_TABLE} downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_details_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kudos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_zones_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate tables setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_columns = {\n",
    "  'activities' : [\n",
    "    'id',\n",
    "    'name',\n",
    "    'distance',\n",
    "    'moving_time',\n",
    "    'elapsed_time',\n",
    "    'total_elevation_gain',\n",
    "    'type',\n",
    "    'sport_type',\n",
    "    'workout_type',\n",
    "    'start_date',\n",
    "    'start_date_local',\n",
    "    'timezone',\n",
    "    'achievement_count',\n",
    "    'kudos_count',\n",
    "    'comment_count',\n",
    "    'athlete_count',\n",
    "    'photo_count',\n",
    "    'trainer',\n",
    "    'commute',\n",
    "    'manual',\n",
    "    'visibility',\n",
    "    'start_latlng',\n",
    "    'end_latlng',\n",
    "    'average_speed',\n",
    "    'max_speed',\n",
    "    'average_cadence',\n",
    "    'average_watts',\n",
    "    'max_watts',\n",
    "    'weighted_average_watts',\n",
    "    'has_heartrate',\n",
    "    'average_heartrate',\n",
    "    'max_heartrate',\n",
    "    'elev_high',\n",
    "    'elev_low',\n",
    "    'pr_count',\n",
    "    'total_photo_count',\n",
    "    'suffer_score',\n",
    "    'description',\n",
    "    'calories',\n",
    "    'device_name',\n",
    "    'map_id',\n",
    "    'gear_id'],\n",
    "  'maps' : [\n",
    "    'map_id',\n",
    "    'map_polyline',\n",
    "    'map_summary_polyline'],\n",
    "  'gear' : [\n",
    "    'gear_id',\n",
    "    'gear_name',\n",
    "    'gear_distance',\n",
    "    'gear_converted_distance',\n",
    "    'start_date',\n",
    "    'start_date_local'],\n",
    "  'segment_efforts' : [\n",
    "    'id',\n",
    "    'name',\n",
    "    'elapsed_time',\n",
    "    'moving_time',\n",
    "    'start_date',\n",
    "    'start_date_local',\n",
    "    'distance',\n",
    "    'start_index',\n",
    "    'end_index',\n",
    "    'average_cadence',\n",
    "    'device_watts',\n",
    "    'average_watts',\n",
    "    'average_heartrate',\n",
    "    'max_heartrate',\n",
    "    'pr_rank',\n",
    "    'achievements',\n",
    "    'visibility',\n",
    "    'kom_rank',\n",
    "    'hidden',\n",
    "    'activity_id',\n",
    "    'segment_id'],\n",
    "  'segments' : [\n",
    "    'segment_id',\n",
    "    'segment_name',\n",
    "    'segment_activity_type',\n",
    "    'segment_distance',\n",
    "    'segment_average_grade',\n",
    "    'segment_maximum_grade',\n",
    "    'segment_elevation_high',\n",
    "    'segment_elevation_low',\n",
    "    'segment_start_latlng',\n",
    "    'segment_end_latlng',\n",
    "    'segment_elevation_profile',\n",
    "    'segment_elevation_profiles',\n",
    "    'segment_climb_category',\n",
    "    'segment_private',\n",
    "    'segment_hazardous',\n",
    "    'segment_starred'],\n",
    "  'laps' : [\n",
    "    'id',\n",
    "    'name',\n",
    "    'elapsed_time',\n",
    "    'moving_time',\n",
    "    'start_date',\n",
    "    'start_date_local',\n",
    "    'distance',\n",
    "    'average_speed',\n",
    "    'max_speed',\n",
    "    'lap_index',\n",
    "    'split',\n",
    "    'start_index',\n",
    "    'end_index',\n",
    "    'total_elevation_gain',\n",
    "    'average_cadence',\n",
    "    'device_watts',\n",
    "    'average_watts',\n",
    "    'average_heartrate',\n",
    "    'max_heartrate',\n",
    "    'pace_zone',\n",
    "    'activity_id'],\n",
    "  'best_efforts' : [\n",
    "    'id',\n",
    "    'activity_id',\n",
    "    'name',\n",
    "    'elapsed_time',\n",
    "    'moving_time',\n",
    "    'start_date',\n",
    "    'start_date_local',\n",
    "    'distance',\n",
    "    'pr_rank',\n",
    "    'achievements',\n",
    "    'start_index',\n",
    "    'end_index']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting data into tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_cols(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Select only the specified columns from a DataFrame if they exist.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  df : pd.DataFrame\n",
    "      The input DataFrame.\n",
    "  cols : list of str\n",
    "      List of column names to select.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  pd.DataFrame\n",
    "      A new DataFrame containing only the specified columns that exist \n",
    "      in the input DataFrame. If none of the columns exist, \n",
    "      an empty DataFrame is returned.\n",
    "  \"\"\"\n",
    "  \n",
    "  existing = [c for c in cols if c in df.columns]\n",
    "  \n",
    "  return df[existing].copy() if existing else pd.DataFrame()\n",
    "\n",
    "def explode_normalize_json(df: pd.DataFrame, col: str, id_col: str | None = None, id_name: str | None = None) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Explode a list-like column into multiple rows and normalize nested JSON/dict objects \n",
    "  into a flat tabular structure.\n",
    "\n",
    "  This function is useful for columns containing arrays of JSON objects \n",
    "  (e.g. laps, segment efforts). Each element of the list becomes a separate row, \n",
    "  and nested fields are flattened into individual columns. Optionally, \n",
    "  a parent identifier column can be retained/renamed to act as a foreign key.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  df : pd.DataFrame\n",
    "      Input DataFrame containing the column to explode.\n",
    "  col : str\n",
    "      Name of the column with list- or dict-like values to explode and normalize.\n",
    "  id_col : str, optional\n",
    "      Name of the column in the input DataFrame to keep as a parent identifier.\n",
    "      If provided, it will be included in the output.\n",
    "  id_name : str, optional\n",
    "      If provided together with `id_col`, renames the identifier column \n",
    "      in the result (e.g. from \"id\" to \"activity_id\").\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  pd.DataFrame\n",
    "      A new DataFrame where:\n",
    "        * each list element from `col` is a separate row,\n",
    "        * nested JSON/dict objects are flattened into columns with names joined by \"_\",\n",
    "        * the parent identifier (`id_col`) is preserved and optionally renamed.\n",
    "      If `col` is missing or contains only null/empty values, \n",
    "      an empty DataFrame is returned.\n",
    "  \"\"\"\n",
    "\n",
    "  if col not in df.columns:\n",
    "    return pd.DataFrame()\n",
    "  \n",
    "  base_cols = [col]\n",
    "\n",
    "  if id_col and id_col in df.columns:\n",
    "    base_cols.insert(0, id_col)\n",
    "\n",
    "  base = df[base_cols].copy()\n",
    "  exploded = base.explode(col, ignore_index=True)\n",
    "  values = exploded[col].dropna()\n",
    "\n",
    "  if values.empty:\n",
    "    return pd.DataFrame()\n",
    "  \n",
    "  norm = pd.json_normalize(values, sep='_')\n",
    "  out = exploded.loc[values.index].drop(columns=[col]).reset_index(drop=True)\n",
    "  res = pd.concat([out.reset_index(drop=True), norm.reset_index(drop=True)], axis=1)\n",
    "  \n",
    "  if id_col and id_name and id_col in df.columns:\n",
    "    res = res.rename(columns={id_col: id_name})\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activities\n",
    "activities_cols = dataframe_columns['activities']\n",
    "activities_df = select_cols(activities_details_df, activities_cols)\n",
    "logging.info(\"DataFrame 'activities_df' created.\")\n",
    "\n",
    "# Maps\n",
    "maps_cols = dataframe_columns['maps']\n",
    "maps_df = select_cols(activities_details_df, maps_cols)\n",
    "logging.info(\"DataFrame 'maps_df' created.\")\n",
    "\n",
    "# Gear\n",
    "gear_cols = dataframe_columns['gear']\n",
    "gear_df = select_cols(activities_details_df, gear_cols)\n",
    "logging.info(\"DataFrame 'gear_df' created.\")\n",
    "\n",
    "# Segment efforts\n",
    "seg_eff_cols = dataframe_columns['segment_efforts']\n",
    "segments_eff_df = explode_normalize_json(activities_details_df, 'segment_efforts')\n",
    "segments_eff_df = select_cols(segments_eff_df, seg_eff_cols)\n",
    "logging.info(\"DataFrame 'segments_eff_df' created.\")\n",
    "\n",
    "# Segments\n",
    "seg_cols = dataframe_columns['segments']\n",
    "segments_df = explode_normalize_json(activities_details_df, 'segment_efforts')\n",
    "segments_df = select_cols(segments_df, seg_cols)\n",
    "logging.info(\"DataFrame 'segments_df' created.\")\n",
    "\n",
    "# Laps\n",
    "lap_cols = dataframe_columns['laps']\n",
    "laps_df = explode_normalize_json(activities_details_df, 'laps')\n",
    "laps_df = select_cols(laps_df, lap_cols)\n",
    "logging.info(\"DataFrame 'laps_df' created.\")\n",
    "\n",
    "# Best efforts\n",
    "best_eff_cols = dataframe_columns['best_efforts']\n",
    "best_eff_df = explode_normalize_json(activities_details_df, 'best_efforts')\n",
    "best_eff_df = select_cols(best_eff_df, best_eff_cols)\n",
    "logging.info(\"DataFrame 'best_eff_df' created.\")\n",
    "\n",
    "# All dataframes in dictionary\n",
    "dataframes = {\n",
    "    \"activities\": activities_df,\n",
    "    \"maps\": maps_df,\n",
    "    \"gear\": gear_df,\n",
    "    \"segment_efforts\": segments_eff_df,\n",
    "    \"segments\": segments_df,\n",
    "    \"laps\": laps_df,\n",
    "    \"best_efforts\": best_eff_df\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activities Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_to_pace_str(speed: float) -> str | None:\n",
    "  \"\"\"\n",
    "  Convert speed in meters per second to running pace as a string.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  speed : float\n",
    "      Speed value in meters per second. Must be greater than zero.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  str or None\n",
    "      Running pace in the format \"M:SS\" representing minutes per kilometer.\n",
    "      For example, \"5:32\" means 5 minutes and 32 seconds per kilometer.\n",
    "      Returns None if the speed is less than or equal to zero.\n",
    "  \"\"\"\n",
    "\n",
    "  if speed <= 0:\n",
    "    return None\n",
    "  \n",
    "  seconds = 1000/speed\n",
    "  minutes = int(seconds // 60)\n",
    "  sec = int(round(seconds % 60))\n",
    "\n",
    "  if sec == 60:\n",
    "    minutes += 1\n",
    "    sec = 0\n",
    "\n",
    "  return f\"{minutes}:{sec:02d}\"\n",
    "\n",
    "def speed_to_pace_float(speed: float) -> float | None:\n",
    "  \"\"\"\n",
    "  Convert speed in meters per second to running pace as a float.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  speed : float\n",
    "      Speed value in meters per second. Must be greater than zero.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  float or None\n",
    "      Running pace in minutes per kilometer, represented as a float.\n",
    "      For example, 5.53 means approximately 5 minutes and 32 seconds per kilometer.\n",
    "      Returns None if the speed is less than or equal to zero.\n",
    "  \"\"\"\n",
    "\n",
    "  if speed <= 0:\n",
    "    return None\n",
    "  \n",
    "  return 1000 / speed / 60\n",
    "\n",
    "def extract_timedelta(time: pd.Series) -> pd.Series:\n",
    "  \"\"\"\n",
    "  Convert a Series of numeric values (seconds) into timedeltas.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  time : pd.Series\n",
    "      Series containing durations expressed in seconds (int/float). \n",
    "      Null values are preserved as None.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  pd.Series\n",
    "      Series of Python ``datetime.timedelta`` objects. \n",
    "      Each element corresponds to the given number of seconds or None if missing.\n",
    "  \"\"\"\n",
    "\n",
    "  return pd.Series([(timedelta(seconds=int(t)) if pd.notnull(t) else None) for t in time], dtype=\"object\")\n",
    "\n",
    "def extract_latlng(latlng: pd.Series) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Split a Series of latitude/longitude pairs into a DataFrame with separate columns.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  latlng : pd.Series\n",
    "      Series where each element is expected to be a list or tuple of length 2 \n",
    "      (latitude, longitude). If the element is not a valid pair, it is replaced \n",
    "      with [None, None].\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  pd.DataFrame\n",
    "      DataFrame with two columns:\n",
    "        * first column: latitude\n",
    "        * second column: longitude\n",
    "      The index is preserved from the input Series.\n",
    "  \"\"\"\n",
    "  latlng = latlng.apply(\n",
    "    lambda row: row if isinstance(row, (list, tuple)) and len(row) == 2 else [None, None]\n",
    "  )\n",
    "  return pd.DataFrame(latlng.tolist(), index=latlng.index)\n",
    "\n",
    "def etc_gmt_from_offset(minutes: int) -> str:\n",
    "    \"\"\"\n",
    "    Convert a UTC offset (in minutes) to an IANA fixed-offset zone name ``Etc/GMT±N``.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    minutes : int\n",
    "        Offset from UTC in minutes.\n",
    "        Positive values mean UTC+ (east of Greenwich), negative mean UTC− (west).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        IANA timezone name. For whole-hour offsets the format is ``Etc/GMT±H``,\n",
    "        e.g. ``Etc/GMT-2`` for +120 minutes and ``Etc/GMT+5`` for −300 minutes.\n",
    "        For non-hour offsets, minutes are included, e.g. ``Etc/GMT-2:30``.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The ``Etc/GMT`` naming convention uses an **inverted sign** relative to ISO 8601:\n",
    "      ``UTC+02:00 → Etc/GMT-2`` and ``UTC-05:00 → Etc/GMT+5``.\n",
    "    - ``Etc/GMT`` zones are fixed-offset and **do not observe DST**.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> etc_gmt_from_offset(120)\n",
    "    'Etc/GMT-2'\n",
    "    >>> etc_gmt_from_offset(-300)\n",
    "    'Etc/GMT+5'\n",
    "    >>> etc_gmt_from_offset(150)\n",
    "    'Etc/GMT-2:30'\n",
    "    \"\"\"\n",
    "\n",
    "    sign = '-' if minutes > 0 else '+'\n",
    "    h, m = divmod(abs(minutes), 60)\n",
    "    return f\"Etc/GMT{sign}{h}\" if m == 0 else f\"Etc/GMT{sign}{h}:{m:02d}\"\n",
    "\n",
    "def create_datetime_tz_cols(df: pd.DataFrame, date_col: str, date_col_local: str) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Derive a UTC timestamp and a fixed-offset timezone name from UTC and local datetimes.\n",
    "\n",
    "  The function parses a UTC datetime column and a corresponding local datetime column\n",
    "  (both representing the same instant), computes the offset in minutes\n",
    "  ``local - utc``, and maps that offset to a fixed-offset IANA zone name\n",
    "  using ``Etc/GMT±H[:MM]``. It returns a DataFrame with the UTC timestamp and\n",
    "  the derived timezone name.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  df : pd.DataFrame\n",
    "      Input DataFrame containing the UTC and local datetime columns.\n",
    "  date_col : str\n",
    "      Name of the column with UTC datetimes (string or datetime-like).\n",
    "      Values are parsed to a tz-aware UTC dtype.\n",
    "  date_col_local : str\n",
    "      Name of the column with local datetimes (string or datetime-like).\n",
    "      Values are used only to infer the UTC offset.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  pd.DataFrame\n",
    "      A DataFrame with two columns:\n",
    "        * ``start_date_utc_dt`` — tz-aware UTC timestamp (dtype ``datetime64[ns, UTC]``),\n",
    "        * ``tz`` — fixed-offset IANA zone name in the ``Etc/GMT`` family\n",
    "          (e.g., ``\"Etc/GMT-2\"`` for UTC+02:00, ``\"Etc/GMT+5\"`` for UTC−05:00).\n",
    "\n",
    "  Notes\n",
    "  -----\n",
    "  - ``Etc/GMT`` zones are fixed offsets and **do not observe DST**. The sign is\n",
    "    intentionally inverted by IANA naming convention: UTC+02:00 → ``Etc/GMT-2``.\n",
    "  - Both columns must refer to the same moment in time; otherwise the inferred\n",
    "    offset (and thus ``tz``) will be incorrect.\n",
    "\n",
    "  Raises\n",
    "  ------\n",
    "  KeyError\n",
    "      If ``date_col`` or ``date_col_local`` is missing in ``df``.\n",
    "  ValueError\n",
    "      If datetime parsing fails.\n",
    "  \"\"\"\n",
    "  \n",
    "  if date_col not in df.columns or date_col_local not in df.columns:\n",
    "        raise KeyError(f\"Missing required columns: {date_col}, {date_col_local}\")\n",
    "\n",
    "  temp_df = pd.DataFrame()\n",
    "  temp_df[\"start_date_utc_dt\"] = pd.to_datetime(df[date_col])\n",
    "  temp_df[\"start_date_local_dt\"] = pd.to_datetime(df[date_col_local])\n",
    "  temp_df[\"utc_offset\"] = (temp_df[\"start_date_local_dt\"] - temp_df[\"start_date_utc_dt\"]).dt.total_seconds() / 60\n",
    "\n",
    "  temp_df[\"tz\"] = temp_df[\"utc_offset\"].apply(etc_gmt_from_offset)\n",
    "  \n",
    "  return temp_df[[\"start_date_utc_dt\", \"start_date_local_dt\", \"tz\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activity_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_df.loc[:, \"moving_time_td\"]  = extract_timedelta(activities_df[\"moving_time\"])\n",
    "activities_df.loc[:, \"elapsed_time_td\"] = extract_timedelta(activities_df[\"elapsed_time\"])\n",
    "\n",
    "activities_df[[\"start_date_utc_dt\", \"start_date_local_dt\", \"local_timezone\"]] = create_datetime_tz_cols(activities_df, \"start_date\", \"start_date_local\")\n",
    "\n",
    "activities_df[[\"start_lat\", \"start_lng\"]]  = extract_latlng(activities_df[\"start_latlng\"])\n",
    "activities_df[[\"end_lat\", \"end_lng\"]] = extract_latlng(activities_df[\"end_latlng\"])\n",
    "\n",
    "is_run = activities_df['type'] == 'Run'\n",
    "\n",
    "activities_df.loc[is_run, 'average_cadence'] = activities_df['average_cadence'].apply(lambda x: x * 2)\n",
    "\n",
    "activities_df.loc[is_run, 'avg_pace_str'] = activities_df['average_speed'].apply(speed_to_pace_str)\n",
    "activities_df.loc[is_run, 'avg_pace_float'] = activities_df['average_speed'].apply(speed_to_pace_float)\n",
    "\n",
    "activities_df.loc[is_run, 'max_pace_str'] = activities_df['max_speed'].apply(speed_to_pace_str)\n",
    "activities_df.loc[is_run, 'max_pace_float'] = activities_df['max_speed'].apply(speed_to_pace_float)\n",
    "\n",
    "activities_df['gear_id'] = activities_df['gear_id'].fillna(NO_GEAR_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_df['gear_id'] = np.where(\n",
    "    (activities_df['start_date_utc_dt'] < '2020-01-01') &\n",
    "    (activities_df['gear_id'] == 'g9239745'),\n",
    "    NO_GEAR_ID,\n",
    "    activities_df['gear_id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_cols_clean = [\n",
    "    'id',\n",
    "    'name',\n",
    "    'start_date_utc_dt',\n",
    "    'start_date_local_dt',\n",
    "    'local_timezone',\n",
    "    'distance',\n",
    "    'moving_time',\n",
    "    'moving_time_td',\n",
    "    'elapsed_time',\n",
    "    'elapsed_time_td',\n",
    "    'total_elevation_gain',\n",
    "    'elev_low',\n",
    "    'elev_high',\n",
    "    'type',\n",
    "    'sport_type',\n",
    "    'workout_type',\n",
    "    'achievement_count',\n",
    "    'kudos_count',\n",
    "    'comment_count',\n",
    "    'athlete_count',\n",
    "    'photo_count',\n",
    "    'trainer',\n",
    "    'commute',\n",
    "    'manual',\n",
    "    'visibility',\n",
    "    'average_speed',\n",
    "    'avg_pace_str',\n",
    "    'avg_pace_float',\n",
    "    'max_speed',\n",
    "    'max_pace_str',\n",
    "    'max_pace_float',\n",
    "    'average_cadence',\n",
    "    'average_watts',\n",
    "    'max_watts',\n",
    "    'weighted_average_watts',\n",
    "    'has_heartrate',\n",
    "    'average_heartrate',\n",
    "    'max_heartrate',\n",
    "    'pr_count',\n",
    "    'total_photo_count',\n",
    "    'suffer_score',\n",
    "    'description',\n",
    "    'calories',\n",
    "    'device_name',\n",
    "    'start_lat',\n",
    "    'start_lng',\n",
    "    'map_id',\n",
    "    'gear_id'\n",
    "]\n",
    "activities_df = activities_df[activities_cols_clean]\n",
    "activities_df = activities_df.sort_values(by='start_date_utc_dt', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load to PostgreSQL will be made after extracting location from coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maps Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps_df.columns = maps_df.columns.str.replace(\"^map_\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps_cols_clean = [\n",
    "  'id',\n",
    "  'polyline',\n",
    "  'summary_polyline'\n",
    "]\n",
    "maps_df = maps_df[maps_cols_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps_df['latlng'] = maps_df.apply(\n",
    "  lambda row: polyline.decode(row['polyline']) if isinstance(row['polyline'], str) else [], axis=1\n",
    ")\n",
    "maps_df = maps_df.explode('latlng', ignore_index=False)\n",
    "maps_df['point_id'] = maps_df.groupby(level=0).cumcount()\n",
    "maps_df[['lat','lng']] = extract_latlng(maps_df['latlng'])\n",
    "maps_df = maps_df.drop(columns=['polyline', 'summary_polyline', 'latlng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps_df_dtype_map = {\n",
    "    \"id\": String,\n",
    "    \"point_id\": Integer,\n",
    "    \"lat\": Float,\n",
    "    \"lng\": Float\n",
    "}\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_S_SCHEMA};\"))\n",
    "\n",
    "logging.warning(f\"Whole table {TARGET_S_SCHEMA}.{MAPS_S_TABLE} will be overwritten.\")\n",
    "\n",
    "maps_df.to_sql(\n",
    "    name=MAPS_S_TABLE,\n",
    "    schema=TARGET_S_SCHEMA,\n",
    "    con=engine,\n",
    "    if_exists=\"replace\",\n",
    "    index=False,\n",
    "    dtype=maps_df_dtype_map,\n",
    "    method=\"multi\",\n",
    "    chunksize=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gear Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gear_df.columns = gear_df.columns.str.replace(\"^gear_\", \"\", regex=True)\n",
    "gear_df['id'] = gear_df['id'].fillna(NO_GEAR_ID)\n",
    "gear_df['name'] = gear_df['name'].fillna('No gear')\n",
    "gear_df = gear_df.fillna(0)\n",
    "gear_df = gear_df.rename(columns={'distance' : 'distance_m', 'converted_distance' : 'distance_km'})\n",
    "gear_df[[\"start_date_utc_dt\", \"start_date_local_dt\", \"local_timezone\"]] = create_datetime_tz_cols(gear_df, \"start_date\", \"start_date_local\")\n",
    "gear_df = (\n",
    "  gear_df.sort_values(by='start_date_utc_dt', ascending=False)\n",
    "  .drop_duplicates(subset=[\"id\"], keep=\"first\")\n",
    "  .reset_index(drop=True)\n",
    ")\n",
    "gear_df = gear_df[['id', 'name', 'distance_m', 'distance_km']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gear_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gear_df_dtype_map = {\n",
    "    \"id\": String,\n",
    "    \"name\": String,\n",
    "    \"distance_m\": Float,\n",
    "    \"distance_km\": Float,\n",
    "}\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_S_SCHEMA};\"))\n",
    "\n",
    "logging.warning(f\"Whole table {TARGET_S_SCHEMA}.{GEAR_S_TABLE} will be overwritten.\")\n",
    "\n",
    "gear_df.to_sql(\n",
    "    name=GEAR_S_TABLE,\n",
    "    schema=TARGET_S_SCHEMA,\n",
    "    con=engine,\n",
    "    if_exists=\"replace\",\n",
    "    index=False,\n",
    "    dtype=gear_df_dtype_map,\n",
    "    method=\"multi\",\n",
    "    chunksize=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segments efforts Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_types_df = segments_df[['segment_id', 'segment_activity_type']].copy()\n",
    "segments_types_df.drop_duplicates(inplace=True)\n",
    "\n",
    "segments_eff_df = pd.merge(segments_eff_df, segments_types_df, on='segment_id', how='left')\n",
    "segments_eff_df = pd.merge(segments_eff_df.drop(columns=\"achievements\"), explode_normalize_json(segments_eff_df, 'achievements', 'id'), on='id', how='left')\n",
    "\n",
    "segments_eff_df.loc[:, \"moving_time_td\"]  = extract_timedelta(segments_eff_df[\"moving_time\"])\n",
    "segments_eff_df.loc[:, \"elapsed_time_td\"] = extract_timedelta(segments_eff_df[\"elapsed_time\"])\n",
    "\n",
    "segments_eff_df[[\"start_date_utc_dt\", \"start_date_local_dt\", \"local_timezone\"]] = create_datetime_tz_cols(segments_eff_df, \"start_date\", \"start_date_local\")\n",
    "\n",
    "is_run = segments_eff_df['segment_activity_type'] == 'Run'\n",
    "\n",
    "segments_eff_df.loc[is_run, 'average_cadence'] = segments_eff_df['average_cadence'].apply(lambda x: x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_eff_cols_clean = [\n",
    "  'id',\n",
    "  'name',\n",
    "  'start_date_utc_dt',\n",
    "  'start_date_local_dt',\n",
    "  'local_timezone',\n",
    "  'distance',\n",
    "  'moving_time',\n",
    "  'moving_time_td',\n",
    "  'elapsed_time',\n",
    "  'elapsed_time_td',\n",
    "  'average_cadence',\n",
    "  'device_watts',\n",
    "  'average_watts',\n",
    "  'average_heartrate',\n",
    "  'max_heartrate',\n",
    "  'pr_rank',\n",
    "  'visibility',\n",
    "  'kom_rank',\n",
    "  'hidden',\n",
    "  'rank',\n",
    "  'type',\n",
    "  'activity_id',\n",
    "  'segment_id'\n",
    "]\n",
    "segments_eff_df = segments_eff_df[segments_eff_cols_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_eff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_eff_df_dtype_map = {\n",
    "    \"id\": BigInteger,\n",
    "    \"name\": String,\n",
    "    \"start_date_utc_dt\": DateTime(timezone=False),\n",
    "    \"start_date_local_dt\": DateTime(timezone=False),\n",
    "    \"distance\": Float,\n",
    "    \"moving_time\": Integer,\n",
    "    \"moving_time_td\": Interval,\n",
    "    \"elapsed_time\": Integer,\n",
    "    \"elapsed_time_td\": Interval,\n",
    "    \"average_cadence\": Float,\n",
    "    \"device_watts\": Boolean,\n",
    "    \"average_watts\": Float,\n",
    "    \"average_heartrate\": Float,\n",
    "    \"max_heartrate\": Float,\n",
    "    \"pr_rank\": Integer,\n",
    "    \"visibility\": String,\n",
    "    \"kom_rank\": Integer,\n",
    "    \"hidden\": Boolean,\n",
    "    \"rank\": Integer,\n",
    "    \"type\": String,\n",
    "    \"activity_id\": BigInteger,\n",
    "    \"segment_id\": BigInteger\n",
    "}\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_S_SCHEMA};\"))\n",
    "\n",
    "logging.warning(f\"Whole table {TARGET_S_SCHEMA}.{SEGMENTS_EFFORTS_S_TABLE} will be overwritten.\")\n",
    "\n",
    "segments_eff_df.to_sql(\n",
    "    name=SEGMENTS_EFFORTS_S_TABLE,\n",
    "    schema=TARGET_S_SCHEMA,\n",
    "    con=engine,\n",
    "    if_exists=\"replace\",\n",
    "    index=False,\n",
    "    dtype=segments_eff_df_dtype_map,\n",
    "    method=\"multi\",\n",
    "    chunksize=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segments Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_df.columns = segments_df.columns.str.replace(\"^segment_\", \"\", regex=True)\n",
    "\n",
    "segments_df[[\"start_lat\", \"start_lng\"]]  = extract_latlng(segments_df[\"start_latlng\"])\n",
    "segments_df[[\"end_lat\", \"end_lng\"]] = extract_latlng(segments_df[\"end_latlng\"])\n",
    "\n",
    "segments_df = segments_df.drop(columns=[\"start_latlng\", \"end_latlng\"])\n",
    "segments_df = segments_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_df[segments_df['id'] == 20350088]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load to PostgreSQL will be made after extracting location from coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laps Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_types_df = activities_df[['id', 'type']].copy()\n",
    "laps_types_df.drop_duplicates(inplace=True)\n",
    "laps_df = pd.merge(laps_df, laps_types_df, left_on='activity_id', right_on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_df.loc[:, \"moving_time_td\"]  = extract_timedelta(laps_df[\"moving_time\"])\n",
    "laps_df.loc[:, \"elapsed_time_td\"] = extract_timedelta(laps_df[\"elapsed_time\"])\n",
    "\n",
    "laps_df[[\"start_date_utc_dt\", \"start_date_local_dt\", \"local_timezone\"]] = create_datetime_tz_cols(laps_df, \"start_date\", \"start_date_local\")\n",
    "\n",
    "is_run = laps_df['type'] == 'Run'\n",
    "\n",
    "laps_df.loc[is_run, 'average_cadence'] = laps_df['average_cadence'].apply(lambda x: x * 2)\n",
    "\n",
    "laps_df.loc[is_run, 'avg_pace_str'] = laps_df['average_speed'].apply(speed_to_pace_str)\n",
    "laps_df.loc[is_run, 'avg_pace_float'] = laps_df['average_speed'].apply(speed_to_pace_float)\n",
    "\n",
    "laps_df.loc[is_run, 'max_pace_str'] = laps_df['max_speed'].apply(speed_to_pace_str)\n",
    "laps_df.loc[is_run, 'max_pace_float'] = laps_df['max_speed'].apply(speed_to_pace_float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_cols_df_clean = [\n",
    "    'id_x',\n",
    "    'name',\n",
    "    'lap_index',\n",
    "    'split',\n",
    "    'start_date_utc_dt',\n",
    "    'start_date_local_dt',\n",
    "    'local_timezone',\n",
    "    'distance',\n",
    "    'moving_time',\n",
    "    'moving_time_td',\n",
    "    'elapsed_time',\n",
    "    'elapsed_time_td',\n",
    "    'total_elevation_gain',\n",
    "    'type',\n",
    "    'average_speed',\n",
    "    'avg_pace_str',\n",
    "    'avg_pace_float',\n",
    "    'pace_zone',\n",
    "    'max_speed',\n",
    "    'max_pace_str',\n",
    "    'max_pace_float',\n",
    "    'average_cadence',\n",
    "    'device_watts',\n",
    "    'average_watts',\n",
    "    'average_heartrate',\n",
    "    'max_heartrate',\n",
    "    'activity_id'\n",
    "]\n",
    "\n",
    "laps_df = laps_df[laps_cols_df_clean]\n",
    "laps_df = laps_df.rename(columns={'id_x': 'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_df_dtype_map = {\n",
    "\"id\": BigInteger,\n",
    "\"name\": String,\n",
    "\"lap_index\": Integer,\n",
    "\"split\": Integer,\n",
    "\"start_date_utc_dt\": DateTime(timezone=False),\n",
    "\"start_date_local_dt\": DateTime(timezone=False),\n",
    "'local_timezone' : String,\n",
    "\"distance\": Float,\n",
    "\"moving_time\": Integer,\n",
    "\"moving_time_td\": Interval,\n",
    "\"elapsed_time\": Integer,\n",
    "\"elapsed_time_td\": Interval,\n",
    "\"total_elevation_gain\": Float,\n",
    "\"type\": String,\n",
    "\"average_speed\": Float,\n",
    "\"avg_pace_str\": String,\n",
    "\"avg_pace_float\": Float,\n",
    "\"pace_zone\": Float,\n",
    "\"max_speed\": Float,\n",
    "\"max_pace_str\": String,\n",
    "\"max_pace_float\": Float,\n",
    "\"average_cadence\": Float,\n",
    "\"device_watts\": Boolean,\n",
    "\"average_watts\": Float,\n",
    "\"average_heartrate\": Float,\n",
    "\"max_heartrate\": Float,\n",
    "\"activity_id\": BigInteger,\n",
    "}\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_S_SCHEMA};\"))\n",
    "\n",
    "logging.warning(f\"Whole table {TARGET_S_SCHEMA}.{LAPS_S_TABLE} will be overwritten.\")\n",
    "\n",
    "laps_df.to_sql(\n",
    "    name=LAPS_S_TABLE,\n",
    "    schema=TARGET_S_SCHEMA,\n",
    "    con=engine,\n",
    "    if_exists=\"replace\",\n",
    "    index=False,\n",
    "    dtype=laps_df_dtype_map,\n",
    "    method=\"multi\",\n",
    "    chunksize=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best efforts Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_eff_df.loc[:, \"moving_time_td\"]  = extract_timedelta(best_eff_df[\"moving_time\"])\n",
    "best_eff_df.loc[:, \"elapsed_time_td\"] = extract_timedelta(best_eff_df[\"elapsed_time\"])\n",
    "\n",
    "best_eff_df[[\"start_date_utc_dt\", \"start_date_local_dt\", \"local_timezone\"]] = create_datetime_tz_cols(best_eff_df, \"start_date\", \"start_date_local\")\n",
    "best_eff_df = pd.merge(best_eff_df.drop(columns=\"achievements\"), explode_normalize_json(best_eff_df, 'achievements', 'id'), on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eff_df_cols_clean = [\n",
    "    'id',\n",
    "    'name',\n",
    "    'start_date_utc_dt',\n",
    "    'start_date_local_dt',\n",
    "    'local_timezone',\n",
    "    'distance',\n",
    "    'moving_time',\n",
    "    'moving_time_td',\n",
    "    'elapsed_time',\n",
    "    'elapsed_time_td',\n",
    "    'rank',\n",
    "    'type',\n",
    "    'activity_id'\n",
    "]\n",
    "best_eff_df = best_eff_df[best_eff_df_cols_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eff_df_dtype_map = {\n",
    "    \"id\": BigInteger,\n",
    "    \"name\": String,\n",
    "    \"start_date_utc_dt\": DateTime(timezone=False),\n",
    "    \"start_date_local_dt\": DateTime(timezone=False),\n",
    "    \"local_timezone\": String,\n",
    "    \"distance\": Float,\n",
    "    \"moving_time\": Integer,\n",
    "    \"moving_time_td\": Interval,\n",
    "    \"elapsed_time\": Integer,\n",
    "    \"elapsed_time_td\": Interval,\n",
    "    \"rank\": Integer,\n",
    "    \"type\": String,\n",
    "    \"activity_id\": BigInteger\n",
    "}\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_S_SCHEMA};\"))\n",
    "\n",
    "logging.warning(f\"Whole table {TARGET_S_SCHEMA}.{BEST_EFFORTS_S_TABLE} will be overwritten.\")\n",
    "\n",
    "best_eff_df.to_sql(\n",
    "    name=BEST_EFFORTS_S_TABLE,\n",
    "    schema=TARGET_S_SCHEMA,\n",
    "    con=engine,\n",
    "    if_exists=\"replace\",\n",
    "    index=False,\n",
    "    dtype=best_eff_df_dtype_map,\n",
    "    method=\"multi\",\n",
    "    chunksize=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kudos Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "kudos_df['full_name'] = kudos_df[['firstname', 'lastname']].astype('string').agg(' '.join, axis=1)\n",
    "kudos_df_cols_clean = [\n",
    "    'firstname',\n",
    "    'lastname',\n",
    "    'full_name',\n",
    "    'activity_id'\n",
    "]\n",
    "kudos_df = kudos_df[kudos_df_cols_clean]\n",
    "kudos_df = kudos_df.rename(columns={'firstname': 'first_name', 'lastname': 'last_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kudos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kudos_dtype_map = {\n",
    "    \"firs_tname\": String,\n",
    "    \"last_name\": String,\n",
    "    \"full_name\": String,\n",
    "    \"activity_id\": BigInteger\n",
    "}\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_S_SCHEMA};\"))\n",
    "\n",
    "logging.warning(f\"Whole table {TARGET_S_SCHEMA}.{KUDOS_S_TABLE} will be overwritten.\")\n",
    "\n",
    "kudos_df.to_sql(\n",
    "    name=KUDOS_S_TABLE,\n",
    "    schema=TARGET_S_SCHEMA,\n",
    "    con=engine,\n",
    "    if_exists=\"replace\",\n",
    "    index=False,\n",
    "    dtype=kudos_dtype_map,\n",
    "    method=\"multi\",\n",
    "    chunksize=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activities Zones Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zones_sort_add_names(bucket: list, zone_type: str) -> list:\n",
    "  \"\"\"\n",
    "  Sorts a list of zone dicts by their 'min' boundary, assigns a 1-based\n",
    "  'zone_number' to each entry, and sets a human-readable 'zone_name'\n",
    "  based on the provided zone type.\n",
    "\n",
    "  Behavior\n",
    "  --------\n",
    "  - The list is sorted ascending by the 'min' key.\n",
    "  - Each zone dict is mutated in place by adding:\n",
    "      * 'zone_number' (int): 1, 2, 3, ...\n",
    "      * 'zone_name' (str): depends on `type` and the assigned number.\n",
    "  - For type='heartrate':\n",
    "      1→'Z1 - Recovery', 2→'Z2 - Endurance', 3→'Z3 - Tempo',\n",
    "      4→'Z4 - Threshold', 5→'Z5 - Anaerobic'; other numbers → ''.\n",
    "  - For type='pace':\n",
    "      1→'Z1 - Recovery', 2→'Z2 - Endurance', 3→'Z3 - Tempo',\n",
    "      4→'Z4 - Threshold', 5→'Z5 - VO2 Max', 6→'Z6 - Anaerobic';\n",
    "      other numbers → ''.\n",
    "  - Any other `type` produces an empty 'zone_name' for all entries.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  bucket : list[dict]\n",
    "      List of dictionaries representing zones. Each dict must contain\n",
    "      a numeric 'min' key used for sorting.\n",
    "      NOTE: The dictionaries are modified in place.\n",
    "  type : str\n",
    "      Zone classification to use for naming. Expected values:\n",
    "      'heartrate' or 'pace'. Others fall back to empty names.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  list[dict]\n",
    "      A new list with the same dict objects, sorted by 'min' and\n",
    "      with 'zone_number' and 'zone_name' added.\n",
    "\n",
    "  Raises\n",
    "  ------\n",
    "  KeyError\n",
    "      If any dict is missing the 'min' key.\n",
    "  TypeError\n",
    "      If 'min' values are not comparable (e.g., non-numeric types).\n",
    "  \"\"\"\n",
    "\n",
    "  if zone_type not in ['heartrate', 'pace']:\n",
    "        raise ValueError(f\"Unsupported zone_type: {zone_type!r}\")\n",
    "\n",
    "  sorted_bucket = sorted(bucket, key=lambda d: d['min'])\n",
    "\n",
    "  for i, zone in enumerate(sorted_bucket):\n",
    "    zone['zone_number'] = i + 1\n",
    "\n",
    "    if zone_type == 'heartrate':\n",
    "      match zone['zone_number']:\n",
    "        case 1:\n",
    "          zone['zone_name'] = 'Z1 - Recovery'\n",
    "        case 2:\n",
    "          zone['zone_name'] = 'Z2 - Endurance'\n",
    "        case 3:\n",
    "          zone['zone_name'] = 'Z3 - Tempo'\n",
    "        case 4:\n",
    "          zone['zone_name'] = 'Z4 - Threshold'\n",
    "        case 5:\n",
    "          zone['zone_name'] = 'Z5 - Anaerobic'\n",
    "        case _:\n",
    "          zone['zone_name'] = ''\n",
    "    elif zone_type == 'pace':\n",
    "      match zone['zone_number']:\n",
    "        case 1:\n",
    "          zone['zone_name'] = 'Z1 - Recovery'\n",
    "        case 2:\n",
    "          zone['zone_name'] = 'Z2 - Endurance'\n",
    "        case 3:\n",
    "          zone['zone_name'] = 'Z3 - Tempo'\n",
    "        case 4:\n",
    "          zone['zone_name'] = 'Z4 - Threshold'\n",
    "        case 5:\n",
    "          zone['zone_name'] = 'Z5 - VO2 Max'\n",
    "        case 6:\n",
    "          zone['zone_name'] = 'Z6 - Anaerobic'\n",
    "        case _:\n",
    "          zone['zone_name'] = ''\n",
    "    else:\n",
    "      zone['zone_name'] = ''\n",
    "  \n",
    "  return sorted_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_df = activities_zones_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_df = zones_df[zones_df['type'].isin(['heartrate', 'pace'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_df['distribution_buckets_clean'] = zones_df.apply(\n",
    "  lambda row: zones_sort_add_names(row['distribution_buckets'], row['type']), axis=1)\n",
    "zones_df = pd.merge(zones_df.drop(columns=\"distribution_buckets_clean\"), explode_normalize_json(zones_df, 'distribution_buckets_clean', 'id'), on='id', how='left')\n",
    "zones_df['id'] = zones_df[['activity_id', 'type', 'zone_number']].astype('string').agg('-'.join, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_df_cols_clean = [\n",
    "    'id',\n",
    "    'activity_id',\n",
    "    'type',\n",
    "    'zone_number',\n",
    "    'zone_name',\n",
    "    'time',\n",
    "    'min',\n",
    "    'max'\n",
    "]\n",
    "zones_df = zones_df[zones_df_cols_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_dtype_map = {\n",
    "    \"id\": String,\n",
    "    \"activity_id\": BigInteger,\n",
    "    \"type\": String,\n",
    "    'zone_number': Integer,\n",
    "    \"zone_name\": String,\n",
    "    \"time\": Float\n",
    "    \n",
    "}\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_S_SCHEMA};\"))\n",
    "\n",
    "logging.warning(f\"Whole table {TARGET_S_SCHEMA}.{ZONES_S_TABLE} will be overwritten.\")\n",
    "\n",
    "zones_df.to_sql(\n",
    "    name=ZONES_S_TABLE,\n",
    "    schema=TARGET_S_SCHEMA,\n",
    "    con=engine,\n",
    "    if_exists=\"replace\",\n",
    "    index=False,\n",
    "    dtype=zones_dtype_map,\n",
    "    method=\"multi\",\n",
    "    chunksize=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Effort Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_effort_df = activities_zones_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_effort_df = relative_effort_df[relative_effort_df['type'] == 'heartrate']\n",
    "relative_effort_df = relative_effort_df[['activity_id', 'score']]\n",
    "relative_effort_df = relative_effort_df.rename(columns={'score': 'relative_effort'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_effort_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_effort_dtype_map = {\n",
    "    \"activity_id\": BigInteger,\n",
    "    \"relative_effort\": Float\n",
    "}\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_S_SCHEMA};\"))\n",
    "\n",
    "logging.warning(f\"Whole table {TARGET_S_SCHEMA}.{RELATIVE_EFFORT_S_TABLE} will be overwritten.\")\n",
    "\n",
    "relative_effort_df.to_sql(\n",
    "    name=RELATIVE_EFFORT_S_TABLE,\n",
    "    schema=TARGET_S_SCHEMA,\n",
    "    con=engine,\n",
    "    if_exists=\"replace\",\n",
    "    index=False,\n",
    "    dtype=relative_effort_dtype_map,\n",
    "    method=\"multi\",\n",
    "    chunksize=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding coordinates with geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of unique locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lng_points_df = pd.concat([activities_df[['start_lat', 'start_lng']], segments_df[['start_lat', 'start_lng']]]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = Nominatim(user_agent=USER_AGENT)\n",
    "reverse = RateLimiter(geo.reverse, min_delay_seconds=1.1, max_retries=3, error_wait_seconds=5)\n",
    "\n",
    "def cache_key(lat, lng, precision):\n",
    "  \"\"\"\n",
    "  Create a stable cache key for a coordinate cell.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  lat : float\n",
    "      Latitude in decimal degrees.\n",
    "  lng : float\n",
    "      Longitude in decimal degrees.\n",
    "  precision : int\n",
    "      Number of decimal places used to format the coordinates.\n",
    "      This effectively defines the grid cell size (e.g., 2 dp ≈ city level).\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  str\n",
    "      Key formatted as ``\"lat,lng\"`` with fixed precision, e.g. ``\"51.11,17.02\"``.\n",
    "      Using a string avoids floating-point representation issues and works as a JSON key.\n",
    "  \"\"\"\n",
    "\n",
    "  return f\"{lat:.{precision}f},{lng:.{precision}f}\"\n",
    "\n",
    "def pick_locality(addr):\n",
    "  \"\"\"\n",
    "  Select a locality (city/town/village) from a Nominatim ``address`` mapping.\n",
    "\n",
    "  The function returns the first non-empty value in the following order of preference:\n",
    "  ``city`` → ``town`` → ``village`` → ``hamlet`` → ``municipality`` → ``locality`` → ``county`` (fallback).\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  addr : Mapping[str, str]\n",
    "      The ``address`` object from a Nominatim response (``loc.raw['address']``).\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  str or None\n",
    "      Locality name or ``None`` if none of the keys are available.\n",
    "\n",
    "  Notes\n",
    "  -----\n",
    "  Including ``county`` as the last-resort fallback may return a county name\n",
    "  in places where a true locality is missing in OSM data (useful in some countries),\n",
    "  but it can be more general than a town/city.\n",
    "  \"\"\"\n",
    "  \n",
    "  return (addr.get(\"city\") or addr.get(\"town\") or addr.get(\"village\") or addr.get(\"hamlet\") or addr.get(\"municipality\") or addr.get(\"locality\") or addr.get(\"county\"))\n",
    "\n",
    "def pick_region(addr):\n",
    "  \"\"\"\n",
    "  Select a region/state from a Nominatim ``address`` mapping.\n",
    "\n",
    "  The function returns the first non-empty value in the following order of preference:\n",
    "  ``state`` → ``region`` → ``state_district`` → ``province`` → ``county`` (fallback).\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  addr : Mapping[str, str]\n",
    "      The ``address`` object from a Nominatim response (``loc.raw['address']``).\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  str or None\n",
    "      Region/administrative area name or ``None`` if not present.\n",
    "\n",
    "  Notes\n",
    "  -----\n",
    "  In Poland and many countries ``state`` corresponds to the top-level region\n",
    "  (e.g., voivodeship/province). ``county`` is typically a lower level and is used\n",
    "  here only as a fallback for countries where counties act as primary regions.\n",
    "  \"\"\"\n",
    "  \n",
    "  return (addr.get(\"state\") or addr.get(\"region\") or addr.get(\"state_district\") or addr.get(\"province\") or addr.get(\"county\"))\n",
    "\n",
    "def address_fields(reverse_fn, lat, lng):\n",
    "  \"\"\"\n",
    "  Perform reverse geocoding and extract minimal address fields.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  reverse_fn : Callable\n",
    "      A function compatible with ``geopy.Nominatim.reverse`` (optionally wrapped\n",
    "      with ``RateLimiter``) that accepts ``(lat, lng)`` and returns a Location-like\n",
    "      object with a ``.raw`` dict payload.\n",
    "  lat : float\n",
    "      Latitude in decimal degrees.\n",
    "  lng : float\n",
    "      Longitude in decimal degrees.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  dict\n",
    "      Dictionary with three keys:\n",
    "      - ``locality`` : str or None — city/town/village (best available),\n",
    "      - ``region``   : str or None — region/state/province,\n",
    "      - ``country``  : str or None — country name.\n",
    "\n",
    "  Notes\n",
    "  -----\n",
    "  The function calls ``reverse_fn`` with ``language=\"en\"`` and ``addressdetails=True``.\n",
    "  Change the language parameter if localized names are desired.\n",
    "  \"\"\"\n",
    "\n",
    "  loc = reverse_fn((lat, lng), language=\"en\", addressdetails=True)\n",
    "  if not loc:\n",
    "      return {\"locality\": None, \"region\": None, \"country\": None}\n",
    "  address = (loc.raw or {}).get(\"address\", {})\n",
    "  return {\n",
    "      \"locality\": pick_locality(address),\n",
    "      \"region\": pick_region(address),\n",
    "      \"country\": address.get(\"country\")\n",
    "  }\n",
    "\n",
    "def decode_coordinates(coordinates_df: pd.DataFrame, lat_col: str, lng_col: str, cache_path: str, precision: int, geo_reverse_fn) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Append ``locality``, ``region``, and ``country`` to a DataFrame using reverse geocoding with caching.\n",
    "\n",
    "  For each unique coordinate pair (rounded to ``precision`` decimal places), the function performs\n",
    "  a reverse geocode via ``geo_reverse_fn`` and stores results in a JSON cache. Subsequent runs read\n",
    "  from the cache to minimize API calls.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  coordinates_df : pd.DataFrame\n",
    "      Input DataFrame containing coordinate columns.\n",
    "  lat_col : str\n",
    "      Name of the latitude column (e.g., ``\"start_lat\"``).\n",
    "  lng_col : str\n",
    "      Name of the longitude column (e.g., ``\"start_lng\"``).\n",
    "  cache_path : str\n",
    "      File path to the JSON cache. The file will be created/updated as needed.\n",
    "  precision : int\n",
    "      Number of decimal places for rounding coordinates and building the cache key\n",
    "      (e.g., 2 ≈ city-level granularity).\n",
    "  geo_reverse_fn : Callable\n",
    "      Reverse geocoding function (typically a ``RateLimiter(Nominatim.reverse, ...)``).\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  pd.DataFrame\n",
    "      A copy of the input DataFrame with three additional columns:\n",
    "      ``locality``, ``region``, and ``country``.\n",
    "\n",
    "  Raises\n",
    "  ------\n",
    "  KeyError\n",
    "      If the required ``lat_col`` or ``lng_col`` is missing from ``coordinates_df``.\n",
    "  json.JSONDecodeError\n",
    "      If the cache file exists but contains invalid JSON.\n",
    "  Exception\n",
    "      Any exception propagated from the reverse geocoding function or file I/O.\n",
    "\n",
    "  Notes\n",
    "  -----\n",
    "  - The function writes the cache to ``cache_path`` whenever a new key is added.\n",
    "  - A small sleep (``time.sleep(0.5)``) is used per new lookup; adjust to comply with\n",
    "    your provider's rate limits (public Nominatim typically requires ≤1 request/sec).\n",
    "  - Change the language in ``address_fields`` if you need localized names.\n",
    "  \"\"\"\n",
    "\n",
    "  temp_df = coordinates_df.copy()\n",
    "\n",
    "  if lat_col not in temp_df.columns or lng_col not in temp_df.columns:\n",
    "    raise KeyError(f\"Missing required columns: {lat_col}, {lng_col}\")\n",
    "  \n",
    "  temp_df['lat_round'] = temp_df[lat_col].round(precision)\n",
    "  temp_df['lng_round'] = temp_df[lng_col].round(precision)\n",
    "  temp_df = temp_df.dropna(subset=[lat_col, lng_col])\n",
    "  lat_lng_df = temp_df[['lat_round', 'lng_round']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "  cache = {}\n",
    "  if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "      cache = json.load(f)\n",
    "\n",
    "  records = []\n",
    "  for i, row in tqdm(lat_lng_df.iterrows(), total=lat_lng_df.shape[0]):\n",
    "\n",
    "    key = cache_key(row['lat_round'], row['lng_round'], precision)\n",
    "\n",
    "    if key in cache:\n",
    "      resp = cache[key]\n",
    "\n",
    "    else:\n",
    "      resp = address_fields(geo_reverse_fn, row['lat_round'], row['lng_round'])\n",
    "      cache[key] = resp\n",
    "      time.sleep(0.5)\n",
    "\n",
    "      with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cache, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    records.append({\"lat_round\": row['lat_round'], \"lng_round\": row['lng_round'], **resp})\n",
    "\n",
    "  loc_df = pd.DataFrame(records)\n",
    "  \n",
    "  result = temp_df.merge(loc_df, on=[\"lat_round\",\"lng_round\"], how=\"left\")\n",
    "  result = result.drop(columns=[\"lat_round\",\"lng_round\"])\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lng_points_df = decode_coordinates(lat_lng_points_df, 'start_lat', 'start_lng', CACHE_PATH, PRECISION, reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lng_points_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_df = lat_lng_points_df[['locality', 'region', 'country']].copy()\n",
    "locations_df = locations_df.drop_duplicates()\n",
    "locations_df = locations_df.sort_values(by=['country', 'region', 'locality']).reset_index(drop=True)\n",
    "locations_df['location_id'] = 1000 + np.arange(len(locations_df))\n",
    "locations_df = locations_df[['location_id', 'country', 'region', 'locality']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lng_points_df = pd.merge(lat_lng_points_df, locations_df, how='left', on=['country', 'region', 'locality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lng_points_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_df = pd.merge(activities_df, lat_lng_points_df[['start_lat', 'start_lng', 'location_id']], how='left', on=['start_lat', 'start_lng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_df_dtype_map = {\n",
    "    \"id\": BigInteger,\n",
    "    \"name\": String,\n",
    "    \"start_date_utc_dt\": DateTime(timezone=False),\n",
    "    \"start_date_local_dt\": DateTime(timezone=False),\n",
    "    \"local_timezone\":  String,\n",
    "    \"distance\": Float,\n",
    "    \"moving_time\": Integer,\n",
    "    \"moving_time_td\": Interval,\n",
    "    \"elapsed_time\": Integer,\n",
    "    \"elapsed_time_td\": Interval,\n",
    "    \"total_elevation_gain\": Float,\n",
    "    \"elev_low\": Float,\n",
    "    \"elev_high\": Float,\n",
    "    \"type\": String,\n",
    "    \"sport_type\": String,\n",
    "    \"workout_type\": Integer,\n",
    "    \"achievement_count\": Integer,\n",
    "    \"kudos_count\": Integer,\n",
    "    \"comment_count\": Integer,\n",
    "    \"athlete_count\": Integer,\n",
    "    \"photo_count\": Integer,\n",
    "    \"trainer\": Boolean,\n",
    "    \"commute\": Boolean,\n",
    "    \"manual\": Boolean,\n",
    "    \"visibility\": String,\n",
    "    \"average_speed\": Float,\n",
    "    \"avg_pace_str\": String,\n",
    "    \"avg_pace_float\": Float,\n",
    "    \"max_speed\": Float,\n",
    "    \"max_pace_str\": String,\n",
    "    \"max_pace_float\": Float,\n",
    "    \"average_cadence\": Float,\n",
    "    \"average_watts\": Float,\n",
    "    \"max_watts\": Float,\n",
    "    \"weighted_average_watts\": Float,\n",
    "    \"has_heartrate\": Boolean,\n",
    "    \"average_heartrate\": Float,\n",
    "    \"max_heartrate\": Float,\n",
    "    \"pr_count\": Integer,\n",
    "    \"total_photo_count\": Integer,\n",
    "    \"suffer_score\": Float,\n",
    "    \"description\": Text,\n",
    "    \"calories\": Float,\n",
    "    \"device_name\": String,\n",
    "    'start_lat' : Float,\n",
    "    'start_lng' : Float,\n",
    "    \"map_id\": String,\n",
    "    \"gear_id\": String,\n",
    "    \"location_id\": Integer\n",
    "}\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_S_SCHEMA};\"))\n",
    "\n",
    "logging.warning(f\"Whole table {TARGET_S_SCHEMA}.{ACTIVITIES_S_TABLE} will be overwritten.\")\n",
    "\n",
    "activities_df.to_sql(\n",
    "    name=ACTIVITIES_S_TABLE,\n",
    "    schema=TARGET_S_SCHEMA,\n",
    "    con=engine,\n",
    "    if_exists=\"replace\",\n",
    "    index=False,\n",
    "    dtype=activities_df_dtype_map,\n",
    "    method=\"multi\",\n",
    "    chunksize=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_df = pd.merge(segments_df, lat_lng_points_df[['start_lat', 'start_lng',  'location_id']], how='left', on=['start_lat', 'start_lng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_df_dtype_map = {\n",
    "    \"id\": BigInteger,\n",
    "    \"name\": String,\n",
    "    \"activity_type\": String,\n",
    "    \"distance\": Float,\n",
    "    \"average_grade\": Float,\n",
    "    \"maximum_grade\": Float,\n",
    "    \"elevation_high\": Float,\n",
    "    \"elevation_low\": Float,\n",
    "    \"elevation_profile\": Float,\n",
    "    \"elevation_profiles\": Float,\n",
    "    \"climb_category\": Float,\n",
    "    \"private\": Boolean,\n",
    "    \"hazardous\": Boolean,\n",
    "    \"starred\": Boolean,\n",
    "    \"start_lat\": Float,\n",
    "    \"start_lng\": Float,\n",
    "    \"end_lat\": Float,\n",
    "    \"end_lng\": Float,\n",
    "    \"location_id\": Integer\n",
    "}\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_S_SCHEMA};\"))\n",
    "\n",
    "logging.warning(f\"Whole table {TARGET_S_SCHEMA}.{SEGMENTS_S_TABLE} will be overwritten.\")\n",
    "\n",
    "segments_df.to_sql(\n",
    "    name=SEGMENTS_S_TABLE,\n",
    "    schema=TARGET_S_SCHEMA,\n",
    "    con=engine,\n",
    "    if_exists=\"replace\",\n",
    "    index=False,\n",
    "    dtype=segments_df_dtype_map,\n",
    "    method=\"multi\",\n",
    "    chunksize=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_df = locations_df.rename(columns={'location_id': 'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_df_dtype_map = {\n",
    "    \"id\": Integer,\n",
    "    \"locality\": String,\n",
    "    \"region\t\": String,\n",
    "    \"country\": String\n",
    "}\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_S_SCHEMA};\"))\n",
    "\n",
    "logging.warning(f\"Whole table {TARGET_S_SCHEMA}.{LOCATIONS_S_TABLE} will be overwritten.\")\n",
    "\n",
    "locations_df.to_sql(\n",
    "    name=LOCATIONS_S_TABLE,\n",
    "    schema=TARGET_S_SCHEMA,\n",
    "    con=engine,\n",
    "    if_exists=\"replace\",\n",
    "    index=False,\n",
    "    dtype=locations_df_dtype_map,\n",
    "    method=\"multi\",\n",
    "    chunksize=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary keys definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_instructions= [\n",
    "    f\"\"\"CREATE SCHEMA IF NOT EXISTS {TARGET_S_SCHEMA};\"\"\",\n",
    "    # ********** PRIMARY KEYS **********\n",
    "    # --- activities ---\n",
    "    # PK\n",
    "    f\"\"\"\n",
    "    DO $$\n",
    "    BEGIN\n",
    "      IF to_regclass('{TARGET_S_SCHEMA}.{ACTIVITIES_S_TABLE}') IS NOT NULL\n",
    "         AND NOT EXISTS (\n",
    "           SELECT 1 FROM pg_constraint\n",
    "           WHERE conrelid = to_regclass('{TARGET_S_SCHEMA}.{ACTIVITIES_S_TABLE}')\n",
    "             AND contype = 'p'\n",
    "         )\n",
    "      THEN\n",
    "        ALTER TABLE {TARGET_S_SCHEMA}.{ACTIVITIES_S_TABLE}\n",
    "          ADD CONSTRAINT {ACTIVITIES_S_TABLE}_pkey PRIMARY KEY (id);\n",
    "      END IF;\n",
    "    END $$;\n",
    "    \"\"\",\n",
    "    # --- gear ---\n",
    "    # PK\n",
    "    f\"\"\"\n",
    "    DO $$\n",
    "    BEGIN\n",
    "      IF to_regclass('{TARGET_S_SCHEMA}.{GEAR_S_TABLE}') IS NOT NULL\n",
    "         AND NOT EXISTS (\n",
    "           SELECT 1 FROM pg_constraint\n",
    "           WHERE conrelid = to_regclass('{TARGET_S_SCHEMA}.{GEAR_S_TABLE}')\n",
    "             AND contype = 'p'\n",
    "         )\n",
    "      THEN\n",
    "        ALTER TABLE {TARGET_S_SCHEMA}.{GEAR_S_TABLE}\n",
    "          ADD CONSTRAINT {GEAR_S_TABLE}_pkey PRIMARY KEY (id);\n",
    "      END IF;\n",
    "    END $$;\n",
    "    \"\"\",\n",
    "    # --- segments ---\n",
    "    # PK\n",
    "    f\"\"\"\n",
    "    DO $$\n",
    "    BEGIN\n",
    "      IF to_regclass('{TARGET_S_SCHEMA}.{SEGMENTS_S_TABLE}') IS NOT NULL\n",
    "         AND NOT EXISTS (\n",
    "           SELECT 1 FROM pg_constraint\n",
    "           WHERE conrelid = to_regclass('{TARGET_S_SCHEMA}.{SEGMENTS_S_TABLE}')\n",
    "             AND contype = 'p'\n",
    "         )\n",
    "      THEN\n",
    "        ALTER TABLE {TARGET_S_SCHEMA}.{SEGMENTS_S_TABLE}\n",
    "          ADD CONSTRAINT {SEGMENTS_S_TABLE}_pkey PRIMARY KEY (id);\n",
    "      END IF;\n",
    "    END $$;\n",
    "    \"\"\",\n",
    "    # --- laps ---\n",
    "    # PK\n",
    "    f\"\"\"\n",
    "    DO $$\n",
    "    BEGIN\n",
    "      IF to_regclass('{TARGET_S_SCHEMA}.{LAPS_S_TABLE}') IS NOT NULL\n",
    "         AND NOT EXISTS (\n",
    "           SELECT 1 FROM pg_constraint\n",
    "           WHERE conrelid = to_regclass('{TARGET_S_SCHEMA}.{LAPS_S_TABLE}')\n",
    "             AND contype = 'p'\n",
    "         )\n",
    "      THEN\n",
    "        ALTER TABLE {TARGET_S_SCHEMA}.{LAPS_S_TABLE}\n",
    "          ADD CONSTRAINT {LAPS_S_TABLE}_pkey PRIMARY KEY (id);\n",
    "      END IF;\n",
    "    END $$;\n",
    "    \"\"\",\n",
    "    # --- best efforts ---\n",
    "    # PK\n",
    "    f\"\"\"\n",
    "    DO $$\n",
    "    BEGIN\n",
    "      IF to_regclass('{TARGET_S_SCHEMA}.{BEST_EFFORTS_S_TABLE}') IS NOT NULL\n",
    "         AND NOT EXISTS (\n",
    "           SELECT 1 FROM pg_constraint\n",
    "           WHERE conrelid = to_regclass('{TARGET_S_SCHEMA}.{BEST_EFFORTS_S_TABLE}')\n",
    "             AND contype = 'p'\n",
    "         )\n",
    "      THEN\n",
    "        ALTER TABLE {TARGET_S_SCHEMA}.{BEST_EFFORTS_S_TABLE}\n",
    "          ADD CONSTRAINT {BEST_EFFORTS_S_TABLE}_pkey PRIMARY KEY (id);\n",
    "      END IF;\n",
    "    END $$;\n",
    "    \"\"\",\n",
    "    # --- locations ---\n",
    "    # PK\n",
    "    f\"\"\"\n",
    "    DO $$\n",
    "    BEGIN\n",
    "      IF to_regclass('{TARGET_S_SCHEMA}.{LOCATIONS_S_TABLE}') IS NOT NULL\n",
    "         AND NOT EXISTS (\n",
    "           SELECT 1 FROM pg_constraint\n",
    "           WHERE conrelid = to_regclass('{TARGET_S_SCHEMA}.{LOCATIONS_S_TABLE}')\n",
    "             AND contype = 'p'\n",
    "         )\n",
    "      THEN\n",
    "        ALTER TABLE {TARGET_S_SCHEMA}.{LOCATIONS_S_TABLE}\n",
    "          ADD CONSTRAINT {LOCATIONS_S_TABLE}_pkey PRIMARY KEY (id);\n",
    "      END IF;\n",
    "    END $$;\n",
    "    \"\"\",\n",
    "    # --- zones ---\n",
    "    # PK\n",
    "    f\"\"\"\n",
    "    DO $$\n",
    "    BEGIN\n",
    "      IF to_regclass('{TARGET_S_SCHEMA}.{ZONES_S_TABLE}') IS NOT NULL\n",
    "         AND NOT EXISTS (\n",
    "           SELECT 1 FROM pg_constraint\n",
    "           WHERE conrelid = to_regclass('{TARGET_S_SCHEMA}.{ZONES_S_TABLE}')\n",
    "             AND contype = 'p'\n",
    "         )\n",
    "      THEN\n",
    "        ALTER TABLE {TARGET_S_SCHEMA}.{ZONES_S_TABLE}\n",
    "          ADD CONSTRAINT {ZONES_S_TABLE}_pkey PRIMARY KEY (id);\n",
    "      END IF;\n",
    "    END $$;\n",
    "    \"\"\",\n",
    "    # --- relative_effort ---\n",
    "    # PK\n",
    "    f\"\"\"\n",
    "    DO $$\n",
    "    BEGIN\n",
    "      IF to_regclass('{TARGET_S_SCHEMA}.{RELATIVE_EFFORT_S_TABLE}') IS NOT NULL\n",
    "         AND NOT EXISTS (\n",
    "           SELECT 1 FROM pg_constraint\n",
    "           WHERE conrelid = to_regclass('{TARGET_S_SCHEMA}.{RELATIVE_EFFORT_S_TABLE}')\n",
    "             AND contype = 'p'\n",
    "         )\n",
    "      THEN\n",
    "        ALTER TABLE {TARGET_S_SCHEMA}.{RELATIVE_EFFORT_S_TABLE}\n",
    "          ADD CONSTRAINT {RELATIVE_EFFORT_S_TABLE}_pkey PRIMARY KEY (activity_id);\n",
    "      END IF;\n",
    "    END $$;\n",
    "    \"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create constrains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.begin() as conn:\n",
    "    for sql in keys_instructions:\n",
    "        conn.execute(text(sql))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "strava-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
